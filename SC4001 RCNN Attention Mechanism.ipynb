{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a7ea9f1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Installing libraries\n",
    "!pip install transformers datasets evaluate numpy pandas huggingface torch gensim tensorflow\n",
    "\n",
    "from sklearn import preprocessing\n",
    "import torch\n",
    "\n",
    "def set_seed(seed = 0): # Set seed function\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    \n",
    "from IPython.display import clear_output # To remove outputs once done\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f4e98332",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aldan\\AppData\\Local\\Temp\\ipykernel_28268\\2532518818.py:20: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  textEmotion.clean_textEmotion = textEmotion.clean_textEmotion.str.replace(\"[^a-zA-Z#]\", \" \")\n"
     ]
    }
   ],
   "source": [
    "# Importing dataset into pandas dataframe, splitting into features and labels\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "textEmotion = pd.read_csv(\"text_emotion.csv\")\n",
    "\n",
    "## Data preparation ##\n",
    "# Function for removing unwanted text patterns\n",
    "def remove_pattern(input_txt, pattern):\n",
    "    r = re.findall(pattern, input_txt)\n",
    "    for i in r:\n",
    "        input_txt = re.sub(i, '', input_txt)\n",
    "    return input_txt\n",
    "\n",
    "# Removing twitter handles\n",
    "textEmotion['clean_textEmotion'] = np.vectorize(remove_pattern)(textEmotion['content'], \"@[\\w]*\")\n",
    "\n",
    "# Removing punctuations, numbers, special characters\n",
    "textEmotion.clean_textEmotion = textEmotion.clean_textEmotion.str.replace(\"[^a-zA-Z#]\", \" \")\n",
    "\n",
    "# Sliced out features and labels\n",
    "features = textEmotion.loc[:, 'clean_textEmotion']\n",
    "labels = textEmotion.loc[:, 'sentiment'] #Note that these are Series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "85974202",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train test splitting and encoding\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test =  train_test_split(features, labels, test_size=0.3, random_state=42) # splitting into training and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f452f4bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\aldan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "## Tokenisation. Note that we use glove because using AutoTokenizer makes it complex \n",
    "from gensim.models import Word2Vec\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "#tokenising with word2vec\n",
    "train_tokenised = X_train.apply(lambda x: word_tokenize(x) if type(x) == str else x) # for training set\n",
    "test_tokenised = X_test.apply(lambda x: word_tokenize(x) if type(x) == str else x) # for test set\n",
    "## NOTE THAT TOKENISATION OF FEATURES IS FINISHED AT THIS STAGE ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bcc8298e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# installing torchtext to use glove for vectorising\n",
    "!pip install torchtext\n",
    "import torchtext\n",
    "\n",
    "glove = torchtext.vocab.GloVe(name=\"6B\", dim=50, max_vectors=20000)\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cb8a9c3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorising train_tokenised, test_tokenised\n",
    "glove.get_vecs_by_tokens(train_tokenised.to_numpy()[0], lower_case_backup=False)\n",
    "glove.get_vecs_by_tokens(test_tokenised.to_numpy()[0], lower_case_backup=False)\n",
    "\n",
    "# Embedding and padding for TRAINING data\n",
    "train_embedded = train_tokenised.loc[train_tokenised.map(len)!=0].map(glove.get_vecs_by_tokens)\n",
    "\n",
    "train_embedded.shape\n",
    "#goal: torch tensor of shape (27940, 41, 50)\n",
    "\n",
    "start_token = torch.rand((50,))\n",
    "end_token = torch.rand((50,))\n",
    "\n",
    "train_embedded_padded = torch.empty((train_embedded.shape[0], 41, 50))\n",
    "for i in range(len(train_embedded)):\n",
    "    train_embedded_vec = train_embedded.iloc[i]\n",
    "    train_embedded_padded[i, 0, :] = start_token\n",
    "    train_embedded_padded[i, 1:len(train_embedded_vec) + 1, :] = train_embedded_vec\n",
    "    train_embedded_padded[i, len(train_embedded_vec) + 1, :] = end_token\n",
    "    \n",
    "# Embedding and padding for TEST data\n",
    "test_embedded = test_tokenised.loc[test_tokenised.map(len)!=0].map(glove.get_vecs_by_tokens)\n",
    "\n",
    "test_embedded_padded = torch.empty((test_embedded.shape[0], 41, 50))\n",
    "for i in range(len(test_embedded)):\n",
    "    test_embedded_vec = test_embedded.iloc[i]\n",
    "    test_embedded_padded[i, 0, :] = start_token\n",
    "    test_embedded_padded[i, 1:len(test_embedded_vec) + 1, :] = test_embedded_vec\n",
    "    test_embedded_padded[i, len(test_embedded_vec) + 1, :] = end_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "84f0f55a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((28000, 13), (12000, 13))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# encoding labels using one-hot encoding - converting labels into binary matrix\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "encoder = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
    "\n",
    "# reshaping both training and test labels\n",
    "y_train_reshaped = y_train.to_numpy().reshape(-1, 1)\n",
    "y_test_reshaped = y_test.to_numpy().reshape(-1, 1)\n",
    "\n",
    "# fitting one-hot encoder on TRAINING labels\n",
    "encoder.fit(y_train_reshaped)\n",
    "\n",
    "# transforming TRAINING labels into binary matrix\n",
    "y_train_encoded = encoder.transform(y_train_reshaped)\n",
    "y_test_encoded = encoder.transform(y_test_reshaped)\n",
    "\n",
    "### From this cell, we obtain the following encoded LABELS:\n",
    "# y_train_encoded - for training set\n",
    "# y_test_encoded - for test set\n",
    "\n",
    "# To find out which index of the matrix corresponds to which emotion. Note 13 unique emotions\n",
    "y_train_encoded.shape, y_test_encoded.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3ad1c414",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "class TextEmotionDataset(Dataset):\n",
    "    def __init__(self, embeddings_tensor, labels_tensor):\n",
    "        self.embeddings_tensor = embeddings_tensor\n",
    "        self.labels_tensor = labels_tensor\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.embeddings_tensor)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.embeddings_tensor[index], self.labels_tensor[index] # Retrieve the pre-embedded and padded sequence and its corresponding label\n",
    "\n",
    "# Converting labels to tensors, .long used for CrossEntropyLoss\n",
    "train_labels_tensor = torch.tensor(y_train_encoded).long()\n",
    "test_labels_tensor = torch.tensor(y_test_encoded).long()\n",
    "\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "# Creating TextEmotionDataset, DataLoader\n",
    "train_dataset = TextEmotionDataset(train_embedded_padded, train_labels_tensor)\n",
    "test_dataset = TextEmotionDataset(test_embedded_padded, test_labels_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "# # Hyperparameters\n",
    "# vocab_size = 20000  # Size of your vocabulary\n",
    "# embedding_dim = 50  # Dimensions of your word vectors\n",
    "hidden_dim = 128  # LSTM hidden dimensions\n",
    "output_dim = 13  # Number of classes\n",
    "\n",
    "# Flatten the first two dimensions to treat each word embedding as an entry in a list\n",
    "flattened_embeddings = train_embedded_padded.view(-1, 50)\n",
    "\n",
    "# Here you need to ensure that you only have unique rows corresponding to unique words\n",
    "# This code assumes that each word's embedding in `flattened_embeddings` is unique\n",
    "# which might not be the case in a real-world scenario\n",
    "unique_embeddings, indices = torch.unique(flattened_embeddings, return_inverse=True, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e0beb691",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn, optim\n",
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(SelfAttention, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.query = nn.Linear(input_dim, input_dim)\n",
    "        self.key = nn.Linear(input_dim, input_dim)\n",
    "        self.value = nn.Linear(input_dim, input_dim)\n",
    "        self.softmax = nn.Softmax(dim=2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        queries = self.query(x)\n",
    "        keys = self.key(x)\n",
    "        values = self.value(x)\n",
    "        scores = torch.bmm(queries, keys.transpose(1, 2)) / (self.input_dim ** 0.5)\n",
    "        attention = self.softmax(scores)\n",
    "        weighted = torch.bmm(attention, values)\n",
    "        return weighted\n",
    "\n",
    "class RCNNAttention(nn.Module):\n",
    "    def __init__(self, embedding_dim, lstm_size, hidden_size_linear):\n",
    "        super(RCNNAttention, self).__init__()\n",
    "        self.lstm = nn.LSTM(embedding_dim, lstm_size, batch_first=True, bidirectional=True)\n",
    "        self.W = nn.Linear(embedding_dim + 2*lstm_size, hidden_size_linear)\n",
    "        self.fc = nn.Linear(hidden_size_linear, 13)\n",
    "        \n",
    "        self.attention = SelfAttention(embedding_dim)\n",
    "\n",
    "    def forward(self, embedded):\n",
    "        output = self.attention(embedded)\n",
    "        # x = |bs, seq_len, embedding_dim|\n",
    "        output, _ = self.lstm(output)\n",
    "        # output = |bs, seq_len, 2*hidden_size|\n",
    "        output = torch.cat([output, embedded], 2)\n",
    "        # output = |bs, seq_len, embedding_dim + 2*hidden_size|\n",
    "        \n",
    "        output = nn.functional.tanh(self.W(output)).transpose(1, 2)\n",
    "        # output = |bs, seq_len, hidden_size_linear| -> |bs, hidden_size_linear, seq_len|\n",
    "        output = nn.functional.max_pool1d(output, output.size(2)).squeeze(2)\n",
    "        # output = |bs, hidden_size_linear|\n",
    "        output = self.fc(output)\n",
    "        # output = |bs, class_num|\n",
    "        return output\n",
    "\n",
    "model = RCNNAttention(50, hidden_dim, 100)\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "37b94e56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01, Train Loss: 2.165\n",
      "Epoch: 02, Train Loss: 2.160\n",
      "Epoch: 03, Train Loss: 2.157\n",
      "Epoch: 04, Train Loss: 2.153\n",
      "Epoch: 05, Train Loss: 2.150\n",
      "Epoch: 06, Train Loss: 2.147\n",
      "Epoch: 07, Train Loss: 2.140\n",
      "Epoch: 08, Train Loss: 2.140\n",
      "Epoch: 09, Train Loss: 2.134\n",
      "Epoch: 10, Train Loss: 2.129\n",
      "Epoch: 11, Train Loss: 2.125\n",
      "Epoch: 12, Train Loss: 2.117\n",
      "Epoch: 13, Train Loss: 2.109\n",
      "Epoch: 14, Train Loss: 2.107\n",
      "Epoch: 15, Train Loss: 2.100\n",
      "Epoch: 16, Train Loss: 2.112\n",
      "Epoch: 17, Train Loss: 2.096\n",
      "Epoch: 18, Train Loss: 2.096\n",
      "Epoch: 19, Train Loss: 2.133\n",
      "Epoch: 20, Train Loss: 2.110\n",
      "Epoch: 21, Train Loss: 2.094\n",
      "Epoch: 22, Train Loss: 2.039\n",
      "Epoch: 23, Train Loss: 2.032\n",
      "Epoch: 24, Train Loss: 2.025\n",
      "Epoch: 25, Train Loss: 2.022\n",
      "Epoch: 26, Train Loss: 2.018\n",
      "Epoch: 27, Train Loss: 2.016\n",
      "Epoch: 28, Train Loss: 2.013\n",
      "Epoch: 29, Train Loss: 2.010\n",
      "Epoch: 30, Train Loss: 2.009\n",
      "Epoch: 31, Train Loss: 2.007\n",
      "Epoch: 32, Train Loss: 2.006\n",
      "Epoch: 33, Train Loss: 2.005\n",
      "Epoch: 34, Train Loss: 2.003\n",
      "Epoch: 35, Train Loss: 2.002\n",
      "Epoch: 36, Train Loss: 2.001\n",
      "Epoch: 37, Train Loss: 2.000\n",
      "Epoch: 38, Train Loss: 1.998\n",
      "Epoch: 39, Train Loss: 1.997\n",
      "Epoch: 40, Train Loss: 1.996\n",
      "Epoch: 41, Train Loss: 1.996\n",
      "Epoch: 42, Train Loss: 1.995\n",
      "Epoch: 43, Train Loss: 1.995\n",
      "Epoch: 44, Train Loss: 1.994\n",
      "Epoch: 45, Train Loss: 1.993\n",
      "Epoch: 46, Train Loss: 1.993\n",
      "Epoch: 47, Train Loss: 1.991\n",
      "Epoch: 48, Train Loss: 1.991\n",
      "Epoch: 49, Train Loss: 1.990\n",
      "Epoch: 50, Train Loss: 1.989\n"
     ]
    }
   ],
   "source": [
    "# Example train function\n",
    "def train(model, iterator, optimizer, criterion):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    \n",
    "#     text, label = next(iter(iterator))\n",
    "#     if True:\n",
    "    for text, label in iterator:\n",
    "        optimizer.zero_grad()\n",
    "        label = label.to(dtype=torch.float32)\n",
    "        predictions = model(text)\n",
    "        loss = criterion(predictions, label)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator)\n",
    "\n",
    "num_epochs = 50\n",
    "\n",
    "losses = []\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss = train(model, train_loader, optimizer, criterion)\n",
    "    losses.append(float(train_loss))\n",
    "    print(f'Epoch: {epoch+1:02}, Train Loss: {train_loss:.3f}')\n",
    "    if epoch == 20:\n",
    "        optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70dca365",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
